LLMs use tokens instead of characters for several practical reasons:
Computational efficiency: Processing text character-by-character would be extremely slow. Tokens group common character sequences (like "ing" or whole words like "the") into single units, drastically reducing the number of processing steps. Instead of processing "understanding" as 13 separate characters, it might be just 2-3 tokens.
Better semantic representation: Tokens often correspond to meaningful units like words, subwords, or common prefixes/suffixes. This helps the model learn patterns more effectively. The token "run" has more semantic meaning than the individual characters 'r', 'u', 'n'.
Vocabulary management: With characters, you'd only have ~100 possible inputs (letters, numbers, punctuation), which seems simple but actually makes learning complex. With tokens, you typically have 30,000-100,000 vocabulary items that strike a balance between expressiveness and manageability.
Handling multiple languages: Character-based models struggle with languages that don't use spaces between words (like Chinese) or have complex scripts. Token-based approaches can handle these more gracefully by learning appropriate segmentations for each language.
Context window efficiency: LLMs have limited context windows (how much text they can process at once). Using tokens means you can fit more meaningful content in that window. A 4,000 token limit might cover 3,000 words, but only 16,000 characters if processed individually.
The tokenization process uses algorithms like Byte-Pair Encoding (BPE) that find the most frequent character sequences in training data and assign them token IDs. This is why common words are often single tokens, while rare words get split into multiple tokens.
For developers, this means you need to think about token limits rather than character limits when working with LLM APIs, and why certain strings (like code with unusual syntax) might consume more tokens than expected.
